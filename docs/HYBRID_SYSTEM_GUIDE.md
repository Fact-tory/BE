# ğŸš€ Java-Python í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ê°€ì´ë“œ

> Factory BEì˜ Java Backend + Python Crawler í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ ì™„ì „ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

- [ì‹œìŠ¤í…œ ê°œìš”](#-ì‹œìŠ¤í…œ-ê°œìš”)
- [ì•„í‚¤í…ì²˜ êµ¬ì¡°](#%EF%B8%8F-ì•„í‚¤í…ì²˜-êµ¬ì¡°)
- [í†µì‹  í”„ë¡œí† ì½œ](#-í†µì‹ -í”„ë¡œí† ì½œ)
- [ë™ì‹œì„± ì œì–´](#-ë™ì‹œì„±-ì œì–´)
- [ê°œë°œ ê°€ì´ë“œ](#-ê°œë°œ-ê°€ì´ë“œ)
- [ë°°í¬ ë° ìš´ì˜](#-ë°°í¬-ë°-ìš´ì˜)
- [ëª¨ë‹ˆí„°ë§](#-ëª¨ë‹ˆí„°ë§)

## ğŸ¯ ì‹œìŠ¤í…œ ê°œìš”

Factory BEëŠ” **Java Spring Boot**ì™€ **Python**ì˜ ì¥ì ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ğŸ”„ ì—­í•  ë¶„ë‹´

| ì»´í¬ë„ŒíŠ¸ | ê¸°ìˆ ìŠ¤íƒ | ì£¼ìš” ì—­í•  |
|----------|----------|-----------|
| **Backend API** | Java Spring Boot | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, ë°ì´í„° ê´€ë¦¬, ì‚¬ìš©ì API |
| **Crawler Service** | Python + AsyncIO | ì›¹ í¬ë¡¤ë§, ë°ì´í„° ìˆ˜ì§‘ |
| **Message Queue** | RabbitMQ | Java-Python ê°„ ë¹„ë™ê¸° í†µì‹  |
| **Distributed Lock** | Redis | ë™ì‹œì„± ì œì–´, ì¤‘ë³µ ë°©ì§€ |
| **Progress Tracking** | WebSocket + Redis | ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì¶”ì  |

### âœ¨ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì˜ ì¥ì 

1. **ì„±ëŠ¥ ìµœì í™”**: JavaëŠ” ì•ˆì •ì ì¸ API ì„œë¹„ìŠ¤, Pythonì€ ë¹ ë¥¸ í¬ë¡¤ë§
2. **í™•ì¥ì„±**: ì–¸ì–´ë³„ íŠ¹í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© (Spring Security, Playwright)
3. **ìœ ì§€ë³´ìˆ˜ì„±**: ê´€ì‹¬ì‚¬ ë¶„ë¦¬ë¡œ ë…ë¦½ì ì¸ ê°œë°œ ë° ë°°í¬
4. **ì•ˆì •ì„±**: ì¥ì•  ê²©ë¦¬ - í¬ë¡¤ëŸ¬ ë¬¸ì œê°€ API ì„œë¹„ìŠ¤ì— ì˜í–¥ ì—†ìŒ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

```mermaid
graph TB
    subgraph "Client Layer"
        A1[Web Frontend]
        A2[Mobile App]
        A3[Admin Panel]
    end
    
    subgraph "API Gateway"
        B1[Java Spring Boot Backend]
        B2[REST API Controllers]
        B3[WebSocket Endpoints]
    end
    
    subgraph "Message Queue Layer"
        C1[RabbitMQ Broker]
        C2[Request Queue]
        C3[Result Queue]
        C4[Progress Queue]
    end
    
    subgraph "Crawler Service"
        D1[Python Crawler Worker]
        D2[Playwright Browser]
        D3[Naver Crawler Complete]
    end
    
    subgraph "Storage Layer"
        E1[(MySQL - Metadata)]
        E2[(MongoDB - Raw Data)]
        E3[(Redis - Cache & Lock)]
        E4[(OpenSearch - Search)]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    B2 --> C1
    B3 --> E3
    C1 --> D1
    D1 --> D2
    D1 --> C1
    B1 --> E1
    B1 --> E2
    D1 --> E3
    B1 --> E4
```

## ğŸ“¡ í†µì‹  í”„ë¡œí† ì½œ

### ğŸ”„ ë©”ì‹œì§€ í”Œë¡œìš°

```
1. [Frontend] â†’ [Java API] : í¬ë¡¤ë§ ìš”ì²­
2. [Java API] â†’ [RabbitMQ] : í¬ë¡¤ë§ ë©”ì‹œì§€ ë°œí–‰
3. [RabbitMQ] â†’ [Python]  : ë©”ì‹œì§€ ì†Œë¹„ ë° ì²˜ë¦¬
4. [Python]   â†’ [Redis]   : ë¶„ì‚° ë½ íšë“
5. [Python]   â†’ [Browser] : ì‹¤ì œ ì›¹ í¬ë¡¤ë§
6. [Python]   â†’ [RabbitMQ]: ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
7. [Python]   â†’ [RabbitMQ]: ê²°ê³¼ ë°ì´í„° ì „ì†¡
8. [RabbitMQ] â†’ [Java API]: ê²°ê³¼ ìˆ˜ì‹  ë° ì €ì¥
9. [Java API] â†’ [Frontend]: WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
```

### ğŸ“¨ ë©”ì‹œì§€ í¬ë§·

#### í¬ë¡¤ë§ ìš”ì²­ ë©”ì‹œì§€ (Java â†’ Python)

```json
{
  "requestId": "crawl_req_001",
  "requestType": "NAVER_CRAWLING",
  "payload": {
    "sessionId": "session_123",
    "officeId": "001",         // ì—°í•©ë‰´ìŠ¤
    "categoryId": "100",       // ì •ì¹˜
    "maxArticles": 50,
    "maxScrollAttempts": 10,
    "includeContent": true
  },
  "timestamp": 1692000000000
}
```

#### í¬ë¡¤ë§ ê²°ê³¼ ë©”ì‹œì§€ (Python â†’ Java)

```json
{
  "requestId": "crawl_req_001",
  "success": true,
  "data": [
    {
      "title": "ì •ì¹˜ ë‰´ìŠ¤ ì œëª©",
      "content": "ë‰´ìŠ¤ ë³¸ë¬¸ ë‚´ìš©...",
      "url": "https://n.news.naver.com/article/001/123456",
      "authorName": "ê¹€ê¸°ì",
      "publishedAt": "2024-08-13T10:00:00",
      "officeId": "001",
      "categoryId": "100",
      "articleId": "123456",
      "discoveredAt": "2024-08-13T10:05:00Z",
      "responseTime": 1500,
      "source": "python_crawler",
      "metadata": {
        "mediaName": "ì—°í•©ë‰´ìŠ¤"
      }
    }
  ],
  "errorMessage": null,
  "timestamp": 1692000060000
}
```

#### ì§„í–‰ìƒí™© ë©”ì‹œì§€ (Python â†’ Java)

```json
{
  "sessionId": "session_123",
  "status": "crawling_articles",
  "progress": 65,
  "message": "ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘... (32/50)",
  "totalArticles": 50,
  "processedArticles": 32,
  "successCount": 28,
  "failCount": 4
}
```

## ğŸ”’ ë™ì‹œì„± ì œì–´

### Redis ë¶„ì‚° ë½ ì‹œìŠ¤í…œ

ê°™ì€ ì–¸ë¡ ì‚¬/ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì¤‘ë³µ í¬ë¡¤ë§ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Redis ê¸°ë°˜ ë¶„ì‚° ë½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### Java ì¸¡ ë¶„ì‚° ë½

```java
@RedisLock(
    key = "'naver_crawling:' + #request.officeId + ':' + #request.categoryId",
    waitTime = 5,
    leaseTime = 300,  // 5ë¶„ - í¬ë¡¤ë§ ì‘ì—…ì´ ê¸¸ ìˆ˜ ìˆìŒ
    timeoutMessage = "í•´ë‹¹ ì–¸ë¡ ì‚¬ì˜ í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."
)
public CompletableFuture<List<RawNewsData>> crawlNews(NaverCrawlingRequest request) {
    // í¬ë¡¤ë§ ë¡œì§
}
```

#### Python ì¸¡ ë¶„ì‚° ë½

```python
@redis_lock(
    key_template="naver_crawling:{request.office_id}:{request.category_id}",
    wait_time=5.0,
    lease_time=300.0,
    timeout_message="í•´ë‹¹ ì–¸ë¡ ì‚¬ì˜ í¬ë¡¤ë§ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."
)
async def crawl_news(self, request: NaverCrawlingRequest):
    # í¬ë¡¤ë§ ë¡œì§
```

### ë½ í‚¤ ì •ì±…

| ë½ í‚¤ íŒ¨í„´ | ì„¤ëª… | ì˜ˆì‹œ |
|------------|------|------|
| `naver_crawling:{officeId}:{categoryId}` | ì–¸ë¡ ì‚¬ë³„ ì¹´í…Œê³ ë¦¬ë³„ ë½ | `naver_crawling:001:100` |
| `crawling_progress:{sessionId}` | ì§„í–‰ìƒí™© ìºì‹œ | `crawling_progress:session_123` |

## ğŸ’» ê°œë°œ ê°€ì´ë“œ

### ê°œë°œ í™˜ê²½ ì„¤ì •

#### 1. Docker ì¸í”„ë¼ ì‹œì‘

```bash
# RabbitMQ, Redis, MongoDB, MySQL ë“± ì‹œì‘
docker-compose -f docker-compose.dev.yml up -d

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose -f docker-compose.dev.yml ps
```

#### 2. Java Backend ì‹¤í–‰

```bash
# Gradleë¡œ ì‹¤í–‰
./gradlew bootRun --args='--spring.profiles.active=dev'

# IDEì—ì„œ ì‹¤í–‰ ì‹œ VM Options
-Dspring.profiles.active=dev
```

#### 3. Python Crawler Worker ì‹¤í–‰

```bash
cd python_crawler_advanced

# ì˜ì¡´ì„± ì„¤ì¹˜ (ìµœì´ˆ í•œë²ˆ)
pip install -r requirements.txt
playwright install chromium

# ì›Œì»¤ ì‹¤í–‰
python enhanced_crawler_worker.py
```

### ìƒˆë¡œìš´ í¬ë¡¤ë§ ì†ŒìŠ¤ ì¶”ê°€

#### Java ì¸¡ ì½”ë“œ

```java
// 1. DTO í™•ì¥
public class CustomCrawlingRequest extends CrawlingRequestMessage {
    private String customField;
    // getters, setters
}

// 2. í ì„œë¹„ìŠ¤ì— ë©”ì„œë“œ ì¶”ê°€
@Service
public class CrawlingQueueService {
    
    public CompletableFuture<List<RawNewsData>> submitCustomCrawling(CustomCrawlingRequest request) {
        CrawlingRequestMessage message = CrawlingRequestMessage.builder()
            .requestId(UUID.randomUUID().toString())
            .requestType("CUSTOM_CRAWLING")
            .payload(request)
            .build();
            
        rabbitTemplate.convertAndSend(CRAWLING_EXCHANGE, REQUEST_ROUTING_KEY, message);
        return registerPendingRequest(message.getRequestId());
    }
}
```

#### Python ì¸¡ ì½”ë“œ

```python
# ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì¶”ê°€
class CustomNewsCrawler:
    """ì»¤ìŠ¤í…€ ë‰´ìŠ¤ ì†ŒìŠ¤ í¬ë¡¤ëŸ¬"""
    
    @redis_lock(
        key_template="custom_crawling:{request.source_id}",
        wait_time=5.0,
        lease_time=300.0
    )
    async def crawl_custom_news(self, request):
        # í¬ë¡¤ë§ ë¡œì§ êµ¬í˜„
        pass

# ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ì— ì¶”ê°€
async def handle_crawling_request(self, message_body: bytes):
    message_data = json.loads(message_body.decode())
    request_msg = CrawlingRequestMessage(**message_data)
    
    if request_msg.request_type == "CUSTOM_CRAWLING":
        # ì»¤ìŠ¤í…€ í¬ë¡¤ëŸ¬ ì²˜ë¦¬
        crawler = CustomNewsCrawler()
        results = await crawler.crawl_custom_news(request_msg.payload)
    # ê¸°ì¡´ ë¡œì§...
```

## ğŸš€ ë°°í¬ ë° ìš´ì˜

### Docker ê¸°ë°˜ ë°°í¬

#### docker-compose.prod.yml

```yaml
version: '3.8'

services:
  java-backend:
    build: .
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - RABBITMQ_HOST=rabbitmq
      - REDIS_HOST=redis
    depends_on:
      - rabbitmq
      - redis
      - mysql
      - mongodb

  python-crawler:
    build: ./python_crawler_advanced
    environment:
      - RABBITMQ_URL=amqp://rabbitmq:5672
      - REDIS_URL=redis://redis:6379
    depends_on:
      - rabbitmq
      - redis
    deploy:
      replicas: 3  # í™•ì¥ì„±ì„ ìœ„í•œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤

  rabbitmq:
    image: rabbitmq:3.12-management
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=secure_password
    
  redis:
    image: redis:7.0-alpine
    command: redis-server --requirepass secure_redis_pass
    
  # ê¸°íƒ€ ì„œë¹„ìŠ¤...
```

### í™•ì¥ ì „ëµ

#### Horizontal Scaling

1. **Python Crawler Worker**: 
   - ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¡œ í™•ì¥ ê°€ëŠ¥
   - RabbitMQì˜ ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ë¶€í•˜ ë¶„ì‚°

2. **Java Backend**: 
   - ë¡œë“œ ë°¸ëŸ°ì„œ ë’¤ì— ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°°ì¹˜
   - Redis ì„¸ì…˜ìœ¼ë¡œ ìƒíƒœ ê³µìœ 

#### Vertical Scaling

1. **ë©”ëª¨ë¦¬ í• ë‹¹**:
   - Java Backend: 2-4GB
   - Python Crawler: 1-2GB per instance
   - Playwright: ì¶”ê°€ 500MB per browser

2. **CPU í• ë‹¹**:
   - CPU ì§‘ì•½ì  ì‘ì—…ì€ Python í¬ë¡¤ëŸ¬
   - I/O ì§‘ì•½ì  ì‘ì—…ì€ Java ë°±ì—”ë“œ

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### í•µì‹¬ ë©”íŠ¸ë¦­

#### ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­

```yaml
# Prometheus + Grafana ì„¤ì •
java_backend:
  - api_response_time
  - active_sessions
  - database_connection_pool
  - memory_usage

python_crawler:
  - crawling_success_rate
  - average_crawling_time
  - concurrent_browser_instances
  - memory_per_worker

message_queue:
  - queue_depth
  - message_processing_rate
  - connection_count
  - message_durability

redis_locks:
  - active_locks_count
  - lock_wait_time
  - lock_acquisition_rate
  - expired_locks
```

#### ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­

```yaml
crawling_performance:
  - articles_per_minute
  - success_rate_by_media
  - duplicate_detection_rate
  - content_quality_score

user_engagement:
  - api_calls_per_user
  - popular_search_terms
  - real_time_session_count
```

### ì•Œë¦¼ ì •ì±…

#### Critical Alerts (ì¦‰ì‹œ ì•Œë¦¼)

```yaml
- name: "Crawler Service Down"
  condition: python_crawler_instances == 0
  severity: critical

- name: "Message Queue Overflow"
  condition: rabbitmq_queue_depth > 10000
  severity: critical

- name: "Database Connection Failure"
  condition: mysql_connection_errors > 50
  severity: critical
```

#### Warning Alerts (5ë¶„ ì§€ì—°)

```yaml
- name: "High Crawling Failure Rate"
  condition: crawling_success_rate < 80%
  severity: warning

- name: "Redis Lock Timeout"
  condition: redis_lock_timeouts > 10/min
  severity: warning
```

### ë¡œê·¸ ê´€ë¦¬

#### êµ¬ì¡°í™”ëœ ë¡œê¹…

```json
// Java Backend ë¡œê·¸ í˜•ì‹
{
  "timestamp": "2024-08-13T10:00:00Z",
  "level": "INFO",
  "service": "java-backend",
  "traceId": "trace-123",
  "message": "Crawling request submitted",
  "metadata": {
    "requestId": "crawl_req_001",
    "officeId": "001",
    "categoryId": "100"
  }
}

// Python Crawler ë¡œê·¸ í˜•ì‹
{
  "timestamp": "2024-08-13T10:00:30Z",
  "level": "INFO", 
  "service": "python-crawler",
  "traceId": "trace-123",
  "message": "Article crawling completed",
  "metadata": {
    "sessionId": "session_123",
    "articlesCount": 25,
    "successRate": 0.92
  }
}
```

### í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

#### Java Backend

```http
GET /actuator/health
{
  "status": "UP",
  "components": {
    "rabbitmq": {"status": "UP"},
    "redis": {"status": "UP"}, 
    "mysql": {"status": "UP"},
    "mongodb": {"status": "UP"}
  }
}
```

#### Python Crawler

```http
GET /health
{
  "status": "healthy",
  "uptime": "2h 30m 15s",
  "active_crawlers": 3,
  "processed_requests": 1247,
  "success_rate": 0.94
}
```

## ğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. RabbitMQ ì—°ê²° ë¬¸ì œ

```bash
# ì—°ê²° ìƒíƒœ í™•ì¸
docker exec factory-rabbitmq-dev rabbitmqctl status

# í ìƒíƒœ í™•ì¸
docker exec factory-rabbitmq-dev rabbitmqctl list_queues

# ì—°ê²° ì¬ì„¤ì •
docker-compose -f docker-compose.dev.yml restart rabbitmq
```

#### 2. Redis ë¶„ì‚° ë½ ë¬¸ì œ

```bash
# í™œì„± ë½ í™•ì¸
docker exec factory-redis-dev redis-cli KEYS "naver_crawling:*"

# íŠ¹ì • ë½ ì‚­ì œ (ì£¼ì˜: ìˆ˜ë™ìœ¼ë¡œë§Œ)
docker exec factory-redis-dev redis-cli DEL "naver_crawling:001:100"

# ë½ TTL í™•ì¸
docker exec factory-redis-dev redis-cli TTL "naver_crawling:001:100"
```

#### 3. Python í¬ë¡¤ëŸ¬ ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ ì œí•œ
browser = await playwright.chromium.launch(
    args=[
        "--memory-pressure-off",
        "--max_old_space_size=1024",
        "--disable-dev-shm-usage"
    ]
)

# ì£¼ê¸°ì  GC ì‹¤í–‰
import gc
gc.collect()
```

---

> ğŸ’¡ **ì°¸ê³ **: ì´ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì€ ê° ì–¸ì–´ì˜ ì¥ì ì„ ê·¹ëŒ€í™”í•˜ë©´ì„œë„ ì•ˆì •ì ì¸ í†µì‹ ê³¼ ë™ì‹œì„± ì œì–´ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì§„ë‹¨í•˜ê³  í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.