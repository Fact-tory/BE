#!/usr/bin/env python3
"""
ğŸ Python ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì›Œì»¤
RabbitMQë¥¼ í†µí•´ Java ë°±ì—”ë“œì™€ í†µì‹ í•˜ì—¬ í¬ë¡¤ë§ ì‘ì—… ìˆ˜í–‰
"""

import asyncio
import json
import logging
import sys
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any
import traceback
import signal
import os

import aio_pika
import aiohttp
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from pydantic import BaseModel, Field

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== ë°ì´í„° ëª¨ë¸ ====================

class NaverCrawlingRequest(BaseModel):
    session_id: str = Field(alias="sessionId")
    office_id: str = Field(alias="officeId")
    category_id: str = Field(alias="categoryId")
    max_articles: int = Field(default=100, alias="maxArticles")
    include_content: bool = Field(default=True, alias="includeContent")

class CrawlingRequestMessage(BaseModel):
    request_id: str = Field(alias="requestId")
    request_type: str = Field(alias="requestType")
    payload: NaverCrawlingRequest
    timestamp: int

class RawNewsData(BaseModel):
    title: str
    content: Optional[str] = None
    url: str
    original_url: Optional[str] = Field(None, alias="originalUrl")
    author_name: Optional[str] = Field(None, alias="authorName")
    published_at: Optional[str] = Field(None, alias="publishedAt")
    discovered_at: str = Field(alias="discoveredAt")
    source: str = "python_crawler"
    category_id: Optional[str] = Field(None, alias="categoryId")
    metadata: Dict[str, Any] = Field(default_factory=dict)

class CrawlingResultMessage(BaseModel):
    request_id: str = Field(alias="requestId")
    success: bool
    data: Optional[List[RawNewsData]] = None
    error_message: Optional[str] = Field(None, alias="errorMessage")
    timestamp: int

class CrawlingProgress(BaseModel):
    session_id: str = Field(alias="sessionId")
    status: str
    progress: int
    message: str
    total_articles: int = Field(alias="totalArticles")
    processed_articles: int = Field(alias="processedArticles")
    success_count: int = Field(alias="successCount")
    fail_count: int = Field(alias="failCount")

# ==================== í¬ë¡¤ë§ ì›Œì»¤ í´ë˜ìŠ¤ ====================

class NaverCrawlingWorker:
    def __init__(self, rabbitmq_url: str = "amqp://localhost"):
        self.rabbitmq_url = rabbitmq_url
        self.connection = None
        self.channel = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.running = True
        
        # í ì„¤ì •
        self.EXCHANGE = "crawling.exchange"
        self.REQUEST_QUEUE = "crawling.request.queue"
        self.RESULT_ROUTING_KEY = "crawling.result"
        self.PROGRESS_ROUTING_KEY = "crawling.progress"
    
    async def initialize(self):
        """RabbitMQ ì—°ê²° ë° Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        try:
            # RabbitMQ ì—°ê²°
            self.connection = await aio_pika.connect_robust(self.rabbitmq_url)
            self.channel = await self.connection.channel()
            await self.channel.set_qos(prefetch_count=1)  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            
            logger.info("RabbitMQ ì—°ê²° ì™„ë£Œ")
            
            # Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™”
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-blink-features=AutomationControlled",
                    "--disable-web-security"
                ]
            )
            
            self.context = await self.browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            
            logger.info("Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def start_consuming(self):
        """í¬ë¡¤ë§ ìš”ì²­ íì—ì„œ ë©”ì‹œì§€ ì†Œë¹„ ì‹œì‘"""
        try:
            queue = await self.channel.declare_queue(self.REQUEST_QUEUE, durable=True)
            
            async with queue.iterator() as queue_iter:
                async for message in queue_iter:
                    if not self.running:
                        break
                        
                    async with message.process():
                        try:
                            await self.handle_crawling_request(message.body)
                        except Exception as e:
                            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            logger.error(traceback.format_exc())
        
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì†Œë¹„ ì‹¤íŒ¨: {e}")
            raise
    
    async def handle_crawling_request(self, message_body: bytes):
        """í¬ë¡¤ë§ ìš”ì²­ ì²˜ë¦¬"""
        try:
            # ë©”ì‹œì§€ íŒŒì‹±
            message_data = json.loads(message_body.decode())
            request_msg = CrawlingRequestMessage(**message_data)
            
            logger.info(f"í¬ë¡¤ë§ ìš”ì²­ ìˆ˜ì‹ : {request_msg.request_id}")
            
            # ì§„í–‰ìƒí™© ì´ˆê¸°í™”
            await self.send_progress(
                request_msg.payload.session_id,
                "started", 0, "í¬ë¡¤ë§ ì‹œì‘...", 0, 0, 0, 0
            )
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
            raw_news_list = await self.crawl_naver_news(request_msg.payload)
            
            # ì„±ê³µ ê²°ê³¼ ì „ì†¡
            result = CrawlingResultMessage(
                request_id=request_msg.request_id,
                success=True,
                data=raw_news_list,
                error_message=None,
                timestamp=int(datetime.now().timestamp() * 1000)
            )
            
            await self.send_result(result)
            
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {request_msg.request_id}, ìˆ˜ì§‘ ê¸°ì‚¬: {len(raw_news_list)}")
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            
            # ì‹¤íŒ¨ ê²°ê³¼ ì „ì†¡
            try:
                request_data = json.loads(message_body.decode())
                result = CrawlingResultMessage(
                    request_id=request_data.get("requestId", "unknown"),
                    success=False,
                    data=None,
                    error_message=str(e),
                    timestamp=int(datetime.now().timestamp() * 1000)
                )
                await self.send_result(result)
            except Exception as send_error:
                logger.error(f"ì‹¤íŒ¨ ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {send_error}")
    
    async def crawl_naver_news(self, request: NaverCrawlingRequest) -> List[RawNewsData]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰"""
        page = None
        try:
            page = await self.context.new_page()
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ í˜ì´ì§€ë¡œ ì´ë™
            url = f"https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1={request.category_id}&sid2=&page=1&oid={request.office_id}"
            
            await self.send_progress(
                request.session_id, "crawling", 10, 
                f"ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ ë¡œë”©: {url}", 0, 0, 0, 0
            )
            
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            
            # ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = []
            
            # ìŠ¤í¬ë¡¤ ë° ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            for scroll_attempt in range(5):  # ìµœëŒ€ 5ë²ˆ ìŠ¤í¬ë¡¤
                # í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
                links = await page.eval_on_selector_all(
                    "a[href*='/main/read.naver']", 
                    "elements => elements.map(el => el.href)"
                )
                
                for link in links:
                    if link not in article_links:
                        article_links.append(link)
                
                if len(article_links) >= request.max_articles:
                    break
                
                # ìŠ¤í¬ë¡¤
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")\n                await asyncio.sleep(2)
                
                await self.send_progress(
                    request.session_id, "crawling", 
                    10 + (scroll_attempt + 1) * 10,
                    f"ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘... ({len(article_links)}ê°œ)",
                    len(article_links), 0, 0, 0
                )
            
            # ìƒìœ„ Nê°œ ê¸°ì‚¬ë§Œ ì„ íƒ
            article_links = article_links[:request.max_articles]
            
            await self.send_progress(
                request.session_id, "crawling", 60,
                f"ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ì‹œì‘... ({len(article_links)}ê°œ)",
                len(article_links), 0, 0, 0
            )
            
            # ê° ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§
            raw_news_list = []
            for i, link in enumerate(article_links):
                try:
                    news_data = await self.crawl_single_article(page, link, request)
                    if news_data:
                        raw_news_list.append(news_data)
                    
                    # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                    progress = 60 + ((i + 1) * 35 // len(article_links))
                    await self.send_progress(
                        request.session_id, "crawling", progress,
                        f"ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘... ({i + 1}/{len(article_links)})",
                        len(article_links), i + 1, len(raw_news_list), 
                        (i + 1) - len(raw_news_list)
                    )
                    
                    await asyncio.sleep(0.5)  # ìš”ì²­ ê°„ê²©
                    
                except Exception as e:
                    logger.warning(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {link}, {e}")
            
            await self.send_progress(
                request.session_id, "completed", 100,
                f"í¬ë¡¤ë§ ì™„ë£Œ: {len(raw_news_list)}ê°œ ìˆ˜ì§‘",
                len(article_links), len(article_links), 
                len(raw_news_list), len(article_links) - len(raw_news_list)
            )
            
            return raw_news_list
            
        finally:
            if page:
                await page.close()
    
    async def crawl_single_article(self, page: Page, url: str, request: NaverCrawlingRequest) -> Optional[RawNewsData]:
        """ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            
            # ì œëª© ì¶”ì¶œ
            title = await page.text_content("#title_area span")
            if not title:
                return None
            
            # ë‚´ìš© ì¶”ì¶œ (ì˜µì…˜)
            content = None
            if request.include_content:
                content_element = await page.query_selector("#dic_area, #articleBodyContents")
                if content_element:
                    content = await content_element.text_content()
                    content = self.clean_content(content) if content else None
            
            # ê¸°ìëª… ì¶”ì¶œ
            author_name = None
            author_element = await page.query_selector(".byline_s .by_name, .article_info .by em")
            if author_element:
                author_name = await author_element.text_content()
                if author_name:
                    author_name = author_name.strip().replace("ê¸°ì", "").strip()
            
            # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            media_name = None
            media_element = await page.query_selector(".media_end_head_top_logo img")
            if media_element:
                media_name = await media_element.get_attribute("alt")
            
            # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
            published_at = None
            time_element = await page.query_selector(".article_info span._ARTICLE_DATE_TIME")
            if time_element:
                published_at = await time_element.text_content()
            
            return RawNewsData(
                title=title.strip(),
                content=content,
                url=url,
                original_url=url,
                author_name=author_name,
                published_at=published_at,
                discovered_at=datetime.now(timezone.utc).isoformat(),
                source="python_crawler",
                category_id=request.category_id,
                metadata={
                    "media_name": media_name,
                    "office_id": request.office_id,
                    "crawling_session": request.session_id
                }
            )
            
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {url}, {e}")
            return None
    
    def clean_content(self, content: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ (ìµœì†Œí•œì˜ ì •ì œ)"""
        if not content:
            return ""
        
        # ê¸°ë³¸ì ì¸ ê³µë°± ì •ë¦¬ë§Œ
        content = content.strip()
        content = "\n".join(line.strip() for line in content.split("\n") if line.strip())
        
        return content
    
    async def send_result(self, result: CrawlingResultMessage):
        """í¬ë¡¤ë§ ê²°ê³¼ ì „ì†¡"""
        try:
            message_body = result.json(by_alias=True).encode()
            
            await self.channel.default_exchange.publish(
                aio_pika.Message(
                    body=message_body,
                    content_type="application/json",
                    delivery_mode=aio_pika.DeliveryMode.PERSISTENT
                ),
                routing_key=self.RESULT_ROUTING_KEY
            )
            
            logger.debug(f"ê²°ê³¼ ì „ì†¡ ì™„ë£Œ: {result.request_id}")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    async def send_progress(self, session_id: str, status: str, progress: int, 
                          message: str, total: int, processed: int, 
                          success: int, failed: int):
        """ì§„í–‰ìƒí™© ì „ì†¡"""
        try:
            progress_msg = CrawlingProgress(
                session_id=session_id,
                status=status,
                progress=progress,
                message=message,
                total_articles=total,
                processed_articles=processed,
                success_count=success,
                fail_count=failed
            )
            
            message_body = progress_msg.json(by_alias=True).encode()
            
            await self.channel.default_exchange.publish(
                aio_pika.Message(
                    body=message_body,
                    content_type="application/json"
                ),
                routing_key=self.PROGRESS_ROUTING_KEY
            )
            
        except Exception as e:
            logger.error(f"ì§„í–‰ìƒí™© ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.running = False
        
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        if self.connection:
            await self.connection.close()
        
        logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ==================== ë©”ì¸ ì‹¤í–‰ ====================

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ RabbitMQ URL ê°€ì ¸ì˜¤ê¸°
    rabbitmq_url = os.getenv("RABBITMQ_URL", "amqp://localhost")
    
    worker = NaverCrawlingWorker(rabbitmq_url)
    
    # ì‹ í˜¸ ì²˜ë¦¬ (ìš°ì•„í•œ ì¢…ë£Œ)
    def signal_handler(sig, frame):
        logger.info("ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ , ì‘ì—… ì¤‘ë‹¨...")
        asyncio.create_task(worker.cleanup())
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        await worker.initialize()
        logger.info("Python í¬ë¡¤ë§ ì›Œì»¤ ì‹œì‘")
        await worker.start_consuming()
        
    except Exception as e:
        logger.error(f"ì›Œì»¤ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
    finally:
        await worker.cleanup()

if __name__ == "__main__":
    asyncio.run(main())