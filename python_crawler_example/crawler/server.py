"""FastAPI ì„œë²„ - ë…ë¦½ ì‹¤í–‰ìš©"""

import asyncio
import json
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import uuid

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
import structlog

from .config import settings
from .models import (
    NaverCrawlingRequest,
    CrawlingResult,
    CrawlingStatus,
    RawNewsData
)
from .health import health_checker
from .naver_crawler import NaverNewsCrawler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, settings.log_level.upper()),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = structlog.get_logger(__name__)

# í™œì„± í¬ë¡¤ë§ ì„¸ì…˜ ì¶”ì 
active_sessions: Dict[str, CrawlingResult] = {}
completed_sessions: Dict[str, CrawlingResult] = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    logger.info("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì„œë²„ ì‹œì‘")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(settings.output_dir)
    output_dir.mkdir(exist_ok=True)
    logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir.absolute()}")
    
    # ë°±ê·¸ë¼ìš´ë“œ í—¬ìŠ¤ì²´í¬ ì‹œì‘
    health_task = asyncio.create_task(health_checker.periodic_health_check())
    
    # ì—°ê²° ëª¨ë“œ í™•ì¸
    connection_mode = await health_checker.get_connection_mode()
    logger.info(f"ì—°ê²° ëª¨ë“œ: {connection_mode}")
    
    yield
    
    # ì •ë¦¬ ì‘ì—…
    health_task.cancel()
    await health_checker.close()
    logger.info("ì„œë²„ ì¢…ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ API",
    description="Java ë°±ì—”ë“œì™€ ì—°ë™í•˜ê±°ë‚˜ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì„œë¹„ìŠ¤",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ API",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    connection_mode = await health_checker.get_connection_mode()
    
    return CrawlingStatus(
        active_sessions=list(active_sessions.keys()),
        total_sessions=len(completed_sessions),
        java_backend_connected=health_checker.java_backend_connected,
        rabbitmq_connected=health_checker.rabbitmq_connected,
        system_status=connection_mode
    )


@app.post("/crawl/naver", response_model=CrawlingResult)
async def crawl_naver_news(
    request: NaverCrawlingRequest,
    background_tasks: BackgroundTasks
):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    # ì„¸ì…˜ ID ìƒì„± (ì—†ìœ¼ë©´)
    if not request.session_id:
        request.session_id = str(uuid.uuid4())
    
    session_id = request.session_id
    
    # ì¤‘ë³µ ì„¸ì…˜ ì²´í¬
    if session_id in active_sessions:
        raise HTTPException(
            status_code=409,
            detail=f"ì„¸ì…˜ {session_id}ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤"
        )
    
    logger.info(f"í¬ë¡¤ë§ ìš”ì²­ ìˆ˜ì‹ : session_id={session_id}")
    
    # í¬ë¡¤ë§ ê²°ê³¼ ì´ˆê¸°í™”
    start_time = datetime.now()
    result = CrawlingResult(
        session_id=session_id,
        success=False,
        total_requested=request.max_articles,
        total_found=0,
        total_processed=0,
        success_count=0,
        fail_count=0,
        data=[],
        start_time=start_time,
        end_time=start_time,
        duration_seconds=0.0
    )
    
    active_sessions[session_id] = result
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    background_tasks.add_task(
        execute_crawling,
        request,
        result
    )
    
    return result


@app.get("/sessions/{session_id}", response_model=CrawlingResult)
async def get_session_result(session_id: str):
    """í¬ë¡¤ë§ ì„¸ì…˜ ê²°ê³¼ ì¡°íšŒ"""
    
    # í™œì„± ì„¸ì…˜ í™•ì¸
    if session_id in active_sessions:
        return active_sessions[session_id]
    
    # ì™„ë£Œëœ ì„¸ì…˜ í™•ì¸
    if session_id in completed_sessions:
        return completed_sessions[session_id]
    
    raise HTTPException(
        status_code=404,
        detail=f"ì„¸ì…˜ {session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    )


@app.get("/sessions", response_model=List[str])
async def list_sessions():
    """ëª¨ë“  ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    all_sessions = list(active_sessions.keys()) + list(completed_sessions.keys())
    return sorted(all_sessions, reverse=True)


@app.delete("/sessions/{session_id}")
async def delete_session(session_id: str):
    """ì„¸ì…˜ ì‚­ì œ (ì™„ë£Œëœ ì„¸ì…˜ë§Œ)"""
    if session_id in active_sessions:
        raise HTTPException(
            status_code=409,
            detail="ì‹¤í–‰ ì¤‘ì¸ ì„¸ì…˜ì€ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        )
    
    if session_id in completed_sessions:
        del completed_sessions[session_id]
        return {"message": f"ì„¸ì…˜ {session_id} ì‚­ì œ ì™„ë£Œ"}
    
    raise HTTPException(
        status_code=404,
        detail=f"ì„¸ì…˜ {session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    )


async def execute_crawling(request: NaverCrawlingRequest, result: CrawlingResult):
    """í¬ë¡¤ë§ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)"""
    session_id = request.session_id
    
    try:
        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {session_id}")
        
        # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
        crawler = NaverNewsCrawler()
        await crawler.initialize()
        
        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            raw_news_list = await crawler.crawl_naver_news(request)
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            end_time = datetime.now()
            result.success = True
            result.total_found = len(raw_news_list)
            result.total_processed = len(raw_news_list)
            result.success_count = len(raw_news_list)
            result.data = raw_news_list
            result.end_time = end_time
            result.duration_seconds = (end_time - result.start_time).total_seconds()
            
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {session_id}, ìˆ˜ì§‘: {len(raw_news_list)}ê°œ")
            
            # ë¡œì»¬ ì €ì¥
            if settings.save_raw_data:
                await save_crawling_result(result)
            
        finally:
            await crawler.cleanup()
            
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {session_id}, {e}")
        
        end_time = datetime.now()
        result.success = False
        result.errors.append(str(e))
        result.end_time = end_time
        result.duration_seconds = (end_time - result.start_time).total_seconds()
    
    finally:
        # ì„¸ì…˜ ì´ë™ (í™œì„± â†’ ì™„ë£Œ)
        if session_id in active_sessions:
            completed_sessions[session_id] = active_sessions.pop(session_id)


async def save_crawling_result(result: CrawlingResult):
    """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥"""
    try:
        output_dir = Path(settings.output_dir)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"crawling_{result.session_id}_{timestamp}.json"
        filepath = output_dir / filename
        
        # ê²°ê³¼ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(
                result.model_dump(by_alias=True),
                f,
                ensure_ascii=False,
                indent=2,
                default=str
            )
        
        logger.info(f"í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {filepath}")
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ì„œë²„ ì‹¤í–‰ ì§„ì…ì """
    import uvicorn
    
    uvicorn.run(
        "crawler.server:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
        log_level=settings.log_level.lower()
    )


if __name__ == "__main__":
    main()